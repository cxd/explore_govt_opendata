---
title: "PCA Analysis of ABS Labour Force per Region"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
always_allow_html: yes
---

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
require(ggplot2)
require(MVN)
require(scatterD3)
require(rbokeh)
```

The data set is available from the website [LMIP Gov AU](http://lmip.gov.au/default.aspx?LMIP/Downloads/ABSLabourForceRegion). 

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}

data <- read.csv("data/employment/SA4_regions_feb2017.csv", header=TRUE)

# need to fix the numbers
for(i in 4:9) {
  data[,i] <- as.numeric(gsub(",", "", data[,i]))
}

data1 <- data[,c(1:4)]

colnames(data1) <- c("Region", "State", "Industry", "TotalCount")
data1 <- data1[data1$Region != "Australia",]

head(data1)
temp <- reshape(data1, idvar=c("Region", "State"), timevar=c("Industry"), direction="wide")


temp$Location <- paste(temp$State, temp$Region, sep=" ")
names(temp)
colnames(temp) <- c("Region", 
                 "State",
                 "AGRIC_FRST_FISH", 
                 "MINING",
                 "MANUF",
                 "UTILITIES",
                 "CONSTR",
                 "WSALE_TRADE",
                 "RETAIL_TRADE",
                 "ACC_FOOD_SRV",
                 "TRNS_POST_WHOUSE",
                 "INFO_MEDIA_TELEC",
                 "FIN_INS_SRV",
                 "RENT_HIRE_RE_SRV",
                 "PROF_SCI_TECH_SRV",
                 "ADM_SUP_SRV",
                 "PADMIN_SAFETY",
                 "EDU_TRAIN",
                 "HEALTH_SOC_ASSIST",
                 "ARTS_REC_SRV",
                 "OTHER_SRV",
                 "Location")
df1 <- data.frame(Location=temp$Location,
                  temp[,4:ncol(temp)-1])

X <- df1[,2:ncol(df1)]
X <- scale(X)

row.names(X) <- df1$Location
df1.prcomp <- princomp(X, cor=TRUE)
df1.prcomp$var <- df1.prcomp$sdev^2
```

## Principle components by variance.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
total <- sum(df1.prcomp$var)
df1.prcomp$percentVar <- df1.prcomp$var / total

components <- data.frame(component=1:length(df1.prcomp$var), variance=df1.prcomp$var, percent=round(df1.prcomp$percentVar, 4), cumulativePercent=round(cumsum(df1.prcomp$percentVar),4))

components
```

The first five components explain > 95% of the variation, with the first nine components explaining 99% of the variance.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
ggplot(components) +
  geom_bar(aes(x=component,y=percent),stat="identity")
```

The loadings for each of the industries are shown below.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
df1.prcomp$loadings
```
The following plot provides an ordination of the loadings for locations against the industry weightings as the directions of
the principle components.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}

# plot the first two dimensions.
choices <- c(2,4)

pcobj <- df1.prcomp
nobs.factor <- sqrt(pcobj$n.obs)
d <- pcobj$sdev
u <- sweep(pcobj$scores, 2, 1 / (d * nobs.factor), FUN = '*')
v <- pcobj$loadings
df.u <- as.data.frame(sweep(u[,choices], 2, d[choices], FUN='*'))
 # Directions
  v <- sweep(v, 2, d, FUN='*')
  df.v <- as.data.frame(v[, choices])

  names(df.u) <- c('xvar', 'yvar')
  names(df.v) <- names(df.u)
 
C <- data.frame(x=df.v[,1],
                y=df.v[,2])
O <- data.frame(x=rep(0,nrow(C)),
                y=rep(0,nrow(C)))


figure(width=900,height=900) %>%
  ly_points(x=df.u[,1], y=df.u[,2],
            alpha=0.5) %>%
  ly_text(x=df.u[,1], y=df.u[,2],
          text=rownames(df.u),
          font_size="10pt") %>%
  ly_text(x=df.v[,1], y=df.v[,2],
          text=rownames(df.v), col="red") %>%
  ly_segments(x0=O$x, y0=O$y,
              x1=df.v[,1],
              y1=df.v[,2],
              col="red")
```


That there does seem to be some correspondance between regions and certain industries. We have two codings of data, the first is the long coding (in data1) where industry, and totcal count per industry are allocated between rows, and the second is the wide format with counts captured in the columns with each industry. 

We can look at a log linear model for the data set and examine whether there is suggestion to indicate that the interaction between region and industry is signficant.

```{r}
colnames <- names(data1)
str(data1)
```
We will build a log linear model for this purpose.

```{r}
attach(data1)
```

Note no offset is used in the model below as the total count is the only count factor (we are predicting $n_i$ for each region and industry interaction). 

```{r}
model1 <- glm(TotalCount ~ Region * Industry, family=poisson(link=log))
summary(model1)
```

Note that this is the full model, since there is 0 residual deviance. We use the analysis of deviance to determine whether region does provide a significant explanation of deviance.

```{r}
anova(model1, test="Chisq")
```
Examining the p-values above, we can see that all of the terms, region, industry and the interaction term are all significant in accounting for deviance, hence there is evidence to suggest regional influence on the proportions represented within each industry.

This could be accounted for by differences in population as well at such a summary level, although if we inspect the z-values for the coefficients above we can gain some insight into the significance of the individual interactions. These values are useful given the significance of the interaction term as determined from the analysis of deviance above.

The predictive aspect of this model allows for example an industry and region pair to be selected and the expected proportion of the population to be predicted.

For example, demonstrating the fit of the profession "Professional, Scientific and Technical Services" accross several regions :

```{r}
model2 <- glm(TotalCount ~ Region + Industry, family=poisson(link=log))
```

```{r}
subset <- data1[Industry=="Professional, Scientific and Technical Services",]
detach(data1)
attach(subset)
```

```{r}
p1 <- predict(model1, subset, type="response")
p2 <- predict(model2, subset, type="response")

plot(TotalCount ~ Region, main="Professional, Scientific and Tech services Count per region")
points(p1 ~ Region, col="blue")
points(p2 ~ Region, col="red")
legend("topleft", legend=c("Observation", "Full model", "Reduced Model"), col=c("black", "blue", "red"), pch=c(1,1,1))
```

Note however there is a high level of accuracy in the null model, however the reduced model does have lower accuracy.

We can inspect the scaled residuals for the reduced model (although somewhat less effective for the GLM rather than OLS).

```{r}
par.old <- par(mfrow=c(1,2))
r <- resid(model2)
scalefit <- 2*sqrt(fitted(model2))
plot(r ~ scalefit, ylab="Deviance residuals", xlab="Scaled fitted values")

qqnorm(r)
qqline(r)

par(par.old)

```
We can see the normality assumption is violated as there appears to be influential points at the upper and lower ranges. We can check to determine which points may in fact be highly influential.

Using the cooks distance we can look for leverage points.

```{r, fig.height=8, fig.width=8}
d1 <- cooks.distance(model2)
d2 <- dffits(model2)
hat <- hatvalues(model2)

p <- length(model2$coefficients)
n <- nrow(data1)

h1 <- 2*p/n
h2 <- 2*p/n

c1 <- qf(0.5,p,n-p)

dt <- -2*sqrt(p/n)

par.old <- par(mfrow=c(3,1))
plot(diag(hat), type="h", main="Diagonal of Hat values", ylim=c(0, max(c(diag(hat), h1, h2))))
abline(h=h1, col="red")
abline(h=h2, col="red")
plot(d1, type="h", main="Cooks Distance", ylim=c(0,max(d1, c1)))
abline(h=c1, col="red")
plot(d2, type="h", main="DFFITS", ylim=c(-1*(max(d2, dt)), max(d2, dt)))
abline(h=-dt, col="red")
abline(h=dt, col="red")

```
Based on cooks distance alone it appears that there are a large number of leverage points, however the dffits may be a more reasonable heuristic to look for leverage points. We can select these as follows.

```{r}
test <- which(abs(d2) > abs(dt))
data1[test,]
```

Note that the data points selected via the DFFITS measure (difference in the fitted value standardised), gives some sense as to which combinations of regions and industries can be considered leverage points in the data set. There are a number of possibilities for this, among them the total count for the region and industry pair is very high or very low. 

These points may also correspond to the PCA ordination as being at the extremes of each of the principle components, although visual inspection may be required to confirm this intuition.

We can also compare to the mean counts per region.

```{r}
detach(subset)
```

```{r}
require(dplyr)

means <- data1 %>%
  group_by(Region) %>%
  summarise(MeanTotal=mean(TotalCount))
means


```
```{r, fig.width=8, fig.height=6}
temp <- data1[test,]
temp1 <- merge(means, temp, by="Region")

attach(temp1)
plot(TotalCount ~ Region, main="Distribution of leverages vs total mean count each regions", las=2)
points(MeanTotal ~ Region, col="red")


```
This gives some indication of which regions contain leverage points that are below the regional mean and those which are above.



